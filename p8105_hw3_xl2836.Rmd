---
title: "Homework_3"
author: "Xinyi Lin"
date: "10/11/2018"
output: github_document
---
```{r}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 16,
  fig.asp = .6,
  out.width = "100%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1

## Load and clean data

```{r}
brfss_df = 
  p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, order = TRUE, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
```

## Answer questions

### Question 1

```{r}
brfss_df %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(locations_num = length(unique(locationdesc))) %>% 
  filter(locations_num == 7)
```

According to the table above, we can find that "CT", "FL", "NC" were observed at 7 locations.

### Question 2

```{r}
brfss_df %>% 
  group_by(locationabbr, year) %>% 
  summarise(locations_num = length(unique(locationdesc))) %>% 
  ggplot(aes(x = year, y = locations_num, color = locationabbr)) +
  geom_line() 

head(brfss_df)
```
### Question 3

```{r}
brfss_df %>% 
  filter(year == 2002 | year == 2006 | year == 2010, locationabbr == "NY", response == "Excellent") %>%
  group_by(year) %>% 
  summarise(excellent_mean = mean(data_value),
            excellent_sd = sd(data_value, na.rm = ))
```

### Question 4

```{r}
aver_prop_df =
  brfss_df %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(average_proportion = mean(data_value))

# draw plot
ggplot(aver_prop_df, aes(x = year, y = average_proportion, color = locationabbr)) +
  geom_point() + 
  facet_grid(.~ response) +
  ylab("aver_proportion(%)") +
  theme(legend.position = "none") + 
  theme(axis.text.x = element_text(angle = 45))
```

# Problem 2

## Load and describe data set

```{r}
instacart_df =
  p8105.datasets::instacart %>% 
  janitor::clean_names()

head(instacart_df)
```

The data set "instacart" records the data about orders information, it contains `r nrow(instacart_df)` observations and `r ncol(instacart_df)` variables. Among those variables, `order_id`, `order_number`, `aisle`, `department` are important. For example, for the first observation, product"Bulgarian Yogurt" with id 49302 is the first product add in the number one order by user 112108. This order is the forth order of this user and it is placed at 10. The product comes from aisle "yogurt" and department "dairy eggs".

## Answer Questions

### Question 1

```{r}
asile_num = length(unique(instacart_df$aisle))

aisle_item_df =
  instacart_df %>% 
  group_by(aisle,department) %>% 
  summarise(items_num = n()) %>% 
  arrange(desc(items_num))

aisle_item_df
```

There are `r asile_num` aisles and the "fresh vegetables" aisle is the most items ordered from. As we can find in the table, aisle which more items ordered from are ingredients we need everyday.

### Question 2

```{r}
aisle_item_df %>% 
  as.data.frame() %>% 
  mutate(aisle = reorder(aisle, desc(items_num))) %>% 
  ggplot(aes(x = aisle, y = items_num)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5)) 
```

### Question 3

```{r}
instacart_df %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarise(orders_num = length(unique(order_id))) %>% 
  mutate(rank = min_rank(desc(orders_num))) %>% 
  filter(rank == 1)
  #arrange(aisle, desc(orders_num)) %>% 
  
```

## Question 4

```{r}
instacart_df %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour_day = mean(order_hour_of_day)) %>% 
  mutate(order_dow = factor(order_dow, levels = 0:6,
                      labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))) %>% 
  spread(key = order_dow, value = mean_hour_day)
```

According to the table, we can found that the difference of mean order hour of the day between two items or two days is not significant. And Wednesday is have more orders for two items.

# Problem 3

## Load and describe data

```{r}
ny_noaa_df =
  p8105.datasets::ny_noaa %>% 
  janitor::clean_names()

ny_noaa_df

prcp_na_num =
  ny_noaa_df %>% 
  filter(is.na(prcp)) %>% 
  nrow()

snow_na_num =
  ny_noaa_df %>% 
  filter(is.na(snow)) %>% 
  nrow()

snwd_na_num =
  ny_noaa_df %>% 
  filter(is.na(snwd)) %>% 
  nrow()

tmax_na_num =
  ny_noaa_df %>% 
  filter(is.na(tmax)) %>% 
  nrow()

tmin_na_num =
  ny_noaa_df %>% 
  filter(is.na(tmin)) %>% 
  nrow() 
```

There are `r nrow(ny_noaa_df)` observations and `r ncol(ny_noaa_df)` variables in NY NOAA data, and key variables including `date`, `prcp`, `snow`. In `prcp` variables, the proportion of missing values is `r scales::percent(prcp_na_num/nrow(ny_noaa_df))`. In `snow` variables, the proportion of missing values is `r scales::percent(snow_na_num/nrow(ny_noaa_df))`. In `snwd` variables, the proportion of missing values is `r scales::percent(snwd_na_num/nrow(ny_noaa_df))`. In `tmax` variables, the proportion of missing values is `r scales::percent(tmax_na_num/nrow(ny_noaa_df))`. In `tmin` variables, the proportion of missing values is `r scales::percent(tmin_na_num/nrow(ny_noaa_df))`. There are almost half of values are missing in `tmax` and `tmin` variables, and one fifth of values are missing in `snwd` variable. All of these missing values bring trouble to continuing data analysis.

## Answer questions

### Question 1

First, we need to clean the data.

```{r, cache = TRUE}
ny_noaa_clean = 
  ny_noaa_df %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(year = as.numeric(year),
         month = as.numeric(month),
         day = as.numeric(day),
         prcp = as.numeric(prcp),
         snow = as.numeric(snow),
         snwd = as.numeric(snwd),
         tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10)

ny_noaa_clean
```

Then, we calculate the most commonly observed values for snowfall.

```{r}
table(ny_noaa_clean$snow) %>% 
  data.frame() %>% 
  arrange(desc(Freq))
```

According to the table, we can find 0 is the most commonly observed values. As snow doesn't appear often, no snowfall apears most, so the most commonly observed values is 0.

### Question 2

```{r}
ny_noaa_clean %>% 
  filter(!is.na(tmax), month == 1 | month == 7) %>% 
  mutate(month = month.name[month]) %>% 
  group_by(id, year, month) %>% 
  summarise(aver_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = aver_tmax)) +
  geom_boxplot(aes(group = year)) +
  facet_grid(.~ month)
```

According to the plot, we can find that both "January" table and "July" table have outliers. Tmax in "January"" has a larger range, while the mean of tmax of January is lower.

### Question 3

First table.

```{r}
temp_plot =
  ny_noaa_clean %>% 
  filter(!is.na(tmin), !is.na(tmax)) %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() + 
  theme(legend.text = element_text(angle = 45, size = 6))

temp_plot
```

```{r}
snow_plot =
  ny_noaa_clean %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot(aes(group = year))

snow_plot
```

```{r}
library(patchwork)

temp_plot + snow_plot
```

According to the first plot, we can find, most `tmax` and `tmin` values are around 0. According to the second plot, we can find from 1980 to 1995, the distribution of snowfall values greater than 0 and less than 100 are similar, however, outliers start to appear and  1.5 IQR of the lower quartile and upper quartile are lower after 1997.